{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "- Import the required libraries and helper functions\n",
    "- Load configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import itertools\n",
    "import mlflow\n",
    "from src.data.data_loader import load_data, prepare_data, load_queries\n",
    "from src.data.data_preparation import feature_selection\n",
    "from src.data.utils import downcast_cols, normalization\n",
    "from src.models.clustering import AnomalyDetection\n",
    "from src.models.utils import log_artifact\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "with open('config.yml', 'r') as file:\n",
    "    config=yaml.load(file, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading and Feature Engineering**\n",
    "- Load raw data using specified paths from the configuration file and preprocesses it for modeling\n",
    "- Extracts visitor-level features and applies a feature selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare raw data\n",
    "data = load_data(data_paths = config['data_loader'])\n",
    "data = prepare_data(data = data, config = config['data_preparation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visitor level features and perform feature selection\n",
    "data_features = load_queries(data_paths= config['features'], data= data)\n",
    "features_visitor = data_features['visitor']\n",
    "config['model']['anomaly_detection']['features'] = feature_selection(dataframe = features_visitor)\n",
    "\n",
    "# data normalization\n",
    "features_visitor = normalization(dataframe = features_visitor[config['model']['anomaly_detection']['features']], method = 'min_max')\n",
    "features_visitor = downcast_cols(dataframe = features_visitor)\n",
    "\n",
    "del data, file, data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evalutation and Explainability**\n",
    "- Fit the HDBSCAN model using grid search over multiple parameter combinations\n",
    "- Leverages MLflow for tracking runs, logging metrics, and storing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameter combinations\n",
    "params_space = config['model']['anomaly_detection']['hdbscan']['params']\n",
    "param_combinations = [\n",
    "    dict(zip(params_space.keys(), combo)) for combo in itertools.product(*params_space.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(experiment_name='Anomaly_Detection')\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# open a run for HDBSCAN algorithm\n",
    "with mlflow.start_run(run_name='HDBSCAN'):\n",
    "    for params in param_combinations:\n",
    "        # select hdbscan algo and respective params\n",
    "        hdbscan_Model = AnomalyDetection(method='hdbscan', **params)\n",
    "\n",
    "        # each param combination is logged under a new nested run\n",
    "        with mlflow.start_run(nested=True):\n",
    "\n",
    "            # train algorithm and infer prediction label and its score\n",
    "            hdbscan_Model.fit(np.array(features_visitor))\n",
    "            features_visitor['anomaly_label'] = hdbscan_Model.predict(features_visitor[config['model']['anomaly_detection']['features']])\n",
    "            features_visitor['anomaly_score'] = hdbscan_Model.scoring(features_visitor[config['model']['anomaly_detection']['features']])\n",
    "            features_visitor['outlier_score'] = hdbscan_Model.model.outlier_scores_\n",
    "\n",
    "            # logging config, params and model\n",
    "            mlflow.log_dict(dictionary=config, artifact_file=\"config.yml\")\n",
    "            mlflow.log_params(params=params)\n",
    "            signature = mlflow.models.infer_signature(\n",
    "                model_input = features_visitor[config['model']['anomaly_detection']['features']]\n",
    "                , model_output = features_visitor['anomaly_label']\n",
    "                )\n",
    "            mlflow.sklearn.log_model(sk_model=hdbscan_Model.model, artifact_path='model_instance', signature=signature)\n",
    "\n",
    "            # logging artifacts\n",
    "            artifacts = [\n",
    "                ('datasets', features_visitor, 'visitor_features', None)\n",
    "                , ('stats', hdbscan_Model.model.condensed_tree_.to_pandas(), 'condensed_tree', None)\n",
    "                , ('stats', hdbscan_Model.model.single_linkage_tree_.to_pandas(), 'single_linkage_tree', None)\n",
    "                , ('stats', pd.DataFrame(hdbscan_Model.model.cluster_persistence_, columns=['persistence']), 'cluster persistence', None)\n",
    "            ]\n",
    "            for artifact_path, df, df_name, image_name in artifacts:\n",
    "                log_artifact(artifact_path, df, df_name, image_name)\n",
    "\n",
    "            # logging metrics\n",
    "            mlflow.log_metric(\n",
    "                key = 'silhouette_score_sample'\n",
    "                , value = silhouette_score( X = np.array(features_visitor[features_visitor['anomaly_label']!=-1].iloc[:,:-3])\n",
    "                                           , labels = features_visitor[features_visitor['anomaly_label']!=-1]['anomaly_label']\n",
    "                                           , sample_size = 100000\n",
    "                                           )\n",
    "                                           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
